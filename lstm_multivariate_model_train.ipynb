{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Python libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import lstm_model_modules as lmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the above function on the csv file containing the training data\n",
    "training_file_path = 'train_data.csv'\n",
    "data = lmm.preprocess_csv_file(training_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the scaling factor\n",
    "scaler = lmm.return_scaler(training_file_path)\n",
    "\n",
    "# Scaling only the demand feature in the Pandas dataframe that we got above \n",
    "data['Demand'] *= scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the train dataset length as 80% of the length of the above dataframe\n",
    "train_size = int(len(data) * 0.8)\n",
    "\n",
    "# Splitting the above dataset into train and validation sets:\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the train and validation sets, using a specific lookback range (n_past)\n",
    "# and a specific number of values that we want to predict in the future (n_future)\n",
    "n_future = 1   \n",
    "n_past = 336\n",
    "X_train, Y_train = lmm.create_dataset_updated(train_data, n_future, n_past)\n",
    "X_val, Y_val = lmm.create_dataset_updated(val_data, n_future, n_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False)))\n",
    "model.add(LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "# model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_train.shape[1]))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining early stopping for the model\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# Fitting the model using early stopping\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=16, validation_data=(X_val, Y_val), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model\n",
    "model.save(\"multivariate_lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
